{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mytorch\n",
    "import mytorch.math as math\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class so3:\n",
    "    def __init__(self, n_iter) -> None:\n",
    "        x = torch.zeros(3,3,3,dtype=torch.double)\n",
    "        x[0,2,1] = 1\n",
    "        x[0,1,2] = -1\n",
    "        x[1,0,2] = 1\n",
    "        x[1,2,0] = -1\n",
    "        x[2,1,0] = 1\n",
    "        x[2,0,1] = -1\n",
    "        self.al = x\n",
    "        self.ad = x\n",
    "        \n",
    "        class element_impl(torch.autograd.Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx:torch.autograd.function.FunctionCtx, w:torch.Tensor):\n",
    "                al = self.algebra(w.detach())\n",
    "                ad = self.adjoint(w.detach())\n",
    "                y = torch.matrix_exp(al)\n",
    "                ctx.save_for_backward(y,ad)\n",
    "                return y\n",
    "            \n",
    "            @staticmethod\n",
    "            def backward(ctx:torch.autograd.function.FunctionCtx, dy:torch.Tensor):\n",
    "                y,ad = ctx.saved_tensors\n",
    "                # r = torch.inverse(ad)@(torch.eye(3)-torch.matrix_exp(-ad))\n",
    "                r = torch.eye(3)\n",
    "                p = -ad/2\n",
    "                for i in range(n_iter):\n",
    "                    r += p\n",
    "                    p = -p@ad/(i+3)\n",
    "                dw = torch.einsum('...ij,...ik,...mn,mkj->...n',dy,y,r,self.al)\n",
    "                return dw\n",
    "            \n",
    "        self._element = element_impl.apply\n",
    "            \n",
    "    def algebra(self,w):\n",
    "        return torch.einsum('...i,ijk->...jk',w,self.al)\n",
    "    \n",
    "    def adjoint(self,w):\n",
    "        return torch.einsum('...i,ijk->...jk',w,self.ad)\n",
    "    \n",
    "    def element(self,w):\n",
    "        return self._element(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LieGroup:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    def _create_element_function(self, pade_order):\n",
    "        assert pade_order>=2\n",
    "        k = torch.arange(1,2*pade_order)\n",
    "        c = -torch.cumprod(-1/k,-1)\n",
    "        a,b = math.pade(c,pade_order,pade_order)\n",
    "        \n",
    "        class element_impl(torch.autograd.Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx:torch.autograd.function.FunctionCtx, w:torch.Tensor):\n",
    "                al = self.algebra(w.detach())\n",
    "                y = torch.matrix_exp(al)\n",
    "                ctx.save_for_backward(y,w.detach())\n",
    "                return y\n",
    "            \n",
    "            @staticmethod\n",
    "            def backward(ctx:torch.autograd.function.FunctionCtx, dy:torch.Tensor):\n",
    "                y,w = ctx.saved_tensors\n",
    "                ad = self.adjoint(w)\n",
    "                \n",
    "                p = a[0]*torch.eye(ad.shape[-1],device=dy.device)+a[1]*ad\n",
    "                q = b[0]*torch.eye(ad.shape[-1],device=dy.device)+b[1]*ad\n",
    "                r = ad\n",
    "                for i in range(2,pade_order):\n",
    "                    r = ad@r\n",
    "                    p = p + a[i]*r\n",
    "                    q = q + b[i]*r\n",
    "                r = p@torch.inverse(q)\n",
    "                dw = self.derivative(y, dy.conj())@r\n",
    "                # dw = torch.einsum('...ij,...ik,...mn,mkj->...n',dy,y,r,self.al)\n",
    "                return dw.conj()\n",
    "            \n",
    "        self.__element = element_impl.apply\n",
    "            \n",
    "    def algebra(self, w:torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError(f\"LieGroup [{type(self).__name__}] is missing the required \\\"algebra\\\" function\")\n",
    "    \n",
    "    def adjoint(self, w:torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError(f\"LieGroup [{type(self).__name__}] is missing the required \\\"adjoint\\\" function\")\n",
    "    \n",
    "    def derivative(self, y:torch.Tensor, dy:torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError(f\"LieGroup [{type(self).__name__}] is missing the required \\\"derivative\\\" function\")\n",
    "    \n",
    "    def element(self, w:torch.Tensor) -> torch.Tensor:\n",
    "        return self.__element(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SO3(LieGroup):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        x = torch.zeros(3,3,3,dtype=torch.double)\n",
    "        x[0,2,1] = 1\n",
    "        x[0,1,2] = -1\n",
    "        x[1,0,2] = 1\n",
    "        x[1,2,0] = -1\n",
    "        x[2,1,0] = 1\n",
    "        x[2,0,1] = -1\n",
    "        self.al = x\n",
    "        self.ad = x\n",
    "        self._create_element_function(13)  \n",
    "            \n",
    "    def algebra(self,w):\n",
    "        return torch.einsum('...i,ijk->...jk',w,self.al)\n",
    "    \n",
    "    def adjoint(self,w):\n",
    "        return torch.einsum('...i,ijk->...jk',w,self.ad)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SO(LieGroup):\n",
    "    def __init__(self, n:int, device=None) -> None:\n",
    "        super().__init__()\n",
    "        self.mdim = n\n",
    "        self.gdim = n*(n-1)//2\n",
    "        \n",
    "        al = torch.zeros(self.gdim,self.mdim,self.mdim,dtype=torch.int)\n",
    "        self.__index_al = torch.zeros(self.mdim,self.mdim,dtype=torch.long,device=device)\n",
    "        k = 0\n",
    "        for i in range(1,n):\n",
    "            for j in range(i):\n",
    "                al[k,i,j] = 1\n",
    "                al[k,j,i] = -1\n",
    "                self.__index_al[i,j] = k\n",
    "                self.__index_al[j,i] = k\n",
    "                k += 1\n",
    "        \n",
    "        self.__coef_al = al.sum(0).to(dtype=torch.double,device=device)\n",
    "        self.__index_al = self.__index_al.flatten()\n",
    "        \n",
    "        ad = torch.zeros(self.gdim,self.gdim,self.gdim,dtype=torch.int)\n",
    "        self.__index_ad = torch.zeros(self.gdim,self.gdim,dtype=torch.long,device=device)\n",
    "        for i in range(self.gdim):\n",
    "            ad[i] = self.vectorize(al[i]@al-al@al[i]).transpose(-2,-1)\n",
    "            self.__index_ad[ad[i]!=0] = i\n",
    "        \n",
    "        self.__coef_ad = ad.sum(0).to(dtype=torch.double,device=device)\n",
    "        self.__index_ad = self.__index_ad.flatten()\n",
    "        \n",
    "        self._create_element_function(9)\n",
    "        \n",
    "    def vectorize(self, x:torch.Tensor):\n",
    "        idx0,idx1 = mytorch.count_to_index(torch.arange(self.mdim))\n",
    "        return x[...,idx0,idx1]\n",
    "            \n",
    "    def algebra(self, w:torch.Tensor):\n",
    "        return w[...,self.__index_al].unflatten(-1,(self.mdim,self.mdim))*self.__coef_al\n",
    "        # return torch.einsum('...i,ijk->...jk',w,self.al)\n",
    "    \n",
    "    def adjoint(self, w:torch.Tensor):\n",
    "        return w[...,self.__index_ad].unflatten(-1,(self.gdim,self.gdim))*self.__coef_ad\n",
    "        # return torch.einsum('...i,ijk->...jk',w,self.ad)\n",
    "    \n",
    "    def derivative(self, y:torch.Tensor, dy:torch.Tensor):\n",
    "        return torch.zeros(\n",
    "            y.size()[:-2]+(self.gdim,), \n",
    "            dtype=y.dtype, \n",
    "            device=y.device\n",
    "        ).index_add_(\n",
    "            -1,\n",
    "            self.__index_al,\n",
    "            ((y.transpose(-2,-1)@dy)*self.__coef_al).flatten(-2)\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GW(LieGroup):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._create_element_function(9)\n",
    "        \n",
    "    def coef_al(self, dim:int, device=None):\n",
    "        return -1j*(torch.abs(torch.arange(dim,device=device)[:,None]-torch.arange(dim,device=device)[None,:])<=(dim-1)//2)*\\\n",
    "            torch.arange(-dim//2+1,dim//2+1,device=device,dtype=torch.double)\n",
    "    \n",
    "    def index_al(self, dim:int, device=None):\n",
    "        i = torch.nn.functional.pad(\n",
    "                torch.arange(\n",
    "                    dim,\n",
    "                    device=device,\n",
    "                    dtype=torch.long   \n",
    "                ),\n",
    "                [(dim-1)//2,(dim-1)//2]\n",
    "            )\n",
    "        return torch.as_strided(\n",
    "            i,\n",
    "            size=(dim,dim),\n",
    "            stride=(i.stride(-1),i.stride(-1))\n",
    "        ).flip([-2]).flatten()\n",
    "            \n",
    "    def algebra(self, w:torch.Tensor):\n",
    "        dim = w.shape[-1]\n",
    "        w_ = torch.nn.functional.pad(w,[(dim-1)//2,(dim-1)//2])\n",
    "        return torch.as_strided(\n",
    "            w_,\n",
    "            size=w_.shape[:-1]+(dim,dim),\n",
    "            stride=w_.stride()[:-1]+(w_.stride(-1),w_.stride(-1))\n",
    "        ).flip([-2]).mul(\n",
    "            -1j*torch.arange(-dim//2+1,dim//2+1,device=w.device,dtype=torch.double)\n",
    "        )\n",
    "        \n",
    "    def adjoint(self, w:torch.Tensor):\n",
    "        dim = w.shape[-1]\n",
    "        w_ = torch.nn.functional.pad(w,[(dim-1)//2,(dim-1)//2])\n",
    "        c = torch.arange(-3*(dim-1)//2,3*(dim-1)//2+1,device=w.device,dtype=torch.double)\n",
    "        return torch.as_strided(\n",
    "                w_,\n",
    "                size=w_.shape[:-1]+(dim,dim),\n",
    "                stride=w_.stride()[:-1]+(w_.stride(-1),w_.stride(-1))\n",
    "            ).flip([-1])*\\\n",
    "            torch.as_strided(\n",
    "                -1j*c,\n",
    "                size=(dim,dim),\n",
    "                stride=(c.stride(-1),2*c.stride(-1))\n",
    "            ).flip([-1])\n",
    "    \n",
    "    def derivative(self, y:torch.Tensor, dy:torch.Tensor):\n",
    "        return torch.zeros(\n",
    "            y.size()[:-2]+(y.size(-1),), \n",
    "            dtype=y.dtype, \n",
    "            device=y.device\n",
    "        ).index_add_(\n",
    "            -1,\n",
    "            self.index_al(y.size(-1), device=y.device),\n",
    "            ((y.transpose(-2,-1)@dy)*self.coef_al(y.size(-1), device=y.device)).flatten(-2)\n",
    "        )\n",
    "    \n",
    "    # def element(self, w: torch.Tensor) -> torch.Tensor:\n",
    "    #     return torch.matrix_exp(self.algebra(w))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.+3.j, 0.+2.j, 0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+3.j, 0.+2.j, 0.+1.j, 0.+0.j, 0.-1.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+3.j, 0.+2.j, 0.+1.j, 0.+0.j, 0.-1.j, 0.-2.j, 0.+0.j],\n",
      "        [0.+3.j, 0.+2.j, 0.+1.j, 0.+0.j, 0.-1.j, 0.-2.j, 0.-3.j],\n",
      "        [0.+0.j, 0.+2.j, 0.+1.j, 0.+0.j, 0.-1.j, 0.-2.j, 0.-3.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j, 0.-1.j, 0.-2.j, 0.-3.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.-1.j, 0.-2.j, 0.-3.j]],\n",
      "       dtype=torch.complex128)\n",
      "tensor([3, 4, 5, 6, 0, 0, 0, 2, 3, 4, 5, 6, 0, 0, 1, 2, 3, 4, 5, 6, 0, 0, 1, 2,\n",
      "        3, 4, 5, 6, 0, 0, 1, 2, 3, 4, 5, 0, 0, 0, 1, 2, 3, 4, 0, 0, 0, 0, 1, 2,\n",
      "        3])\n"
     ]
    }
   ],
   "source": [
    "g = GW()\n",
    "print(g.coef_al(7))\n",
    "print(g.index_al(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = GW()\n",
    "size = 513\n",
    "iter = 8\n",
    "w1 = torch.randn(size)+0j\n",
    "w1[1:] = w1[1:] + 1j*torch.randn(size-1)\n",
    "w1 = w1/torch.arange(1,size+1)**2*1e-2\n",
    "w1[16:] = 0\n",
    "w1 = torch.cat([w1[1:].flip([-1]).conj(),w1])\n",
    "w1 = w1.to(dtype=torch.complex128)\n",
    "L1 = g.algebra(w1)\n",
    "\n",
    "w2 = torch.randn(size)+0j\n",
    "w2[1:] = w2[1:] + 1j*torch.randn(size-1)\n",
    "w2 = w2/torch.arange(1,size+1)**2*1e-2\n",
    "w2[16:] = 0\n",
    "w2 = torch.cat([w2[1:].flip([-1]).conj(),w2])\n",
    "w2 = w2.to(dtype=torch.complex128)\n",
    "L2 = g.algebra(w2)\n",
    "\n",
    "CL = L2\n",
    "for i in range(iter):\n",
    "    CL = L1@CL-CL@L1\n",
    "\n",
    "Ad = g.adjoint(w1)\n",
    "w = w2\n",
    "for i in range(iter):\n",
    "    w = Ad@w\n",
    "CA = g.algebra(w)\n",
    "\n",
    "plt.imshow((CA-CL).abs().add(1e-5).log10())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "GradcheckError",
     "evalue": "Jacobian mismatch for output 0 with respect to input 0,\nnumerical:tensor([[-5.8966e-03, -9.9945e-01,  3.0979e-03,  ...,  1.5226e-05,\n          1.1569e-05,  4.6796e-08],\n        [-1.0280e-02, -3.2028e-03, -9.9955e-01,  ..., -1.9792e-05,\n          1.8319e-05,  6.3222e-07],\n        [ 6.0973e-07, -5.0663e-03,  3.0390e-03,  ..., -2.5842e-05,\n         -6.0821e-06,  2.0206e-07],\n        ...,\n        [ 3.2169e-07, -1.5755e-05, -2.3154e-05,  ..., -5.0842e-03,\n          1.2298e-02,  2.8855e-03],\n        [-2.0595e-08, -1.4878e-05,  1.9309e-05,  ...,  9.9967e-01,\n          2.2427e-03,  3.8410e-03],\n        [-4.2927e-07, -1.1387e-05, -1.8292e-05,  ..., -2.2568e-03,\n          9.9968e-01,  4.1756e-04]], dtype=torch.float64)\nanalytical:tensor([[-5.8969e-03, -9.9998e-01,  3.1804e-03,  ...,  4.6177e-05,\n          3.4821e-05,  1.3579e-07],\n        [-1.0282e-02, -3.1191e-03, -9.9995e-01,  ..., -5.9665e-05,\n          5.3867e-05,  1.8906e-06],\n        [ 1.8334e-06, -5.1242e-03,  2.9816e-03,  ..., -7.6937e-05,\n         -1.8028e-05,  6.0946e-07],\n        ...,\n        [ 9.6899e-07, -4.7181e-05, -6.7861e-05,  ..., -5.1301e-03,\n          1.2228e-02,  2.8861e-03],\n        [-6.9706e-08, -4.4127e-05,  5.7611e-05,  ...,  9.9999e-01,\n          2.2483e-03,  3.8414e-03],\n        [-1.2943e-06, -3.4027e-05, -5.5952e-05,  ..., -2.2498e-03,\n          1.0000e+00,  4.1845e-04]], dtype=torch.float64)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGradcheckError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[121], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m g \u001b[39m=\u001b[39m SO(n,device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m      4\u001b[0m w \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(g\u001b[39m.\u001b[39mgdim,requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdouble,device\u001b[39m=\u001b[39mdevice)\u001b[39m*\u001b[39m\u001b[39m1e-2\u001b[39m\n\u001b[0;32m----> 5\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgradcheck(g\u001b[39m.\u001b[39;49melement,w)\n",
      "File \u001b[0;32m~/gitrepo/torchgroup/.venv/lib/python3.8/site-packages/torch/autograd/gradcheck.py:1418\u001b[0m, in \u001b[0;36mgradcheck\u001b[0;34m(func, inputs, eps, atol, rtol, raise_exception, check_sparse_nnz, nondet_tol, check_undefined_grad, check_grad_dtypes, check_batched_grad, check_batched_forward_grad, check_forward_ad, check_backward_ad, fast_mode)\u001b[0m\n\u001b[1;32m   1416\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1417\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1418\u001b[0m     \u001b[39mreturn\u001b[39;00m _gradcheck_helper(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/gitrepo/torchgroup/.venv/lib/python3.8/site-packages/torch/autograd/gradcheck.py:1432\u001b[0m, in \u001b[0;36m_gradcheck_helper\u001b[0;34m(func, inputs, eps, atol, rtol, check_sparse_nnz, nondet_tol, check_undefined_grad, check_grad_dtypes, check_batched_grad, check_batched_forward_grad, check_forward_ad, check_backward_ad, fast_mode)\u001b[0m\n\u001b[1;32m   1429\u001b[0m _check_outputs(outputs)\n\u001b[1;32m   1431\u001b[0m gradcheck_fn \u001b[39m=\u001b[39m _fast_gradcheck \u001b[39mif\u001b[39;00m fast_mode \u001b[39melse\u001b[39;00m _slow_gradcheck\n\u001b[0;32m-> 1432\u001b[0m _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,\n\u001b[1;32m   1433\u001b[0m                      rtol, atol, check_grad_dtypes, check_forward_ad\u001b[39m=\u001b[39;49mcheck_forward_ad,\n\u001b[1;32m   1434\u001b[0m                      check_backward_ad\u001b[39m=\u001b[39;49mcheck_backward_ad, nondet_tol\u001b[39m=\u001b[39;49mnondet_tol,\n\u001b[1;32m   1435\u001b[0m                      check_undefined_grad\u001b[39m=\u001b[39;49mcheck_undefined_grad)\n\u001b[1;32m   1437\u001b[0m \u001b[39mif\u001b[39;00m check_batched_forward_grad:\n\u001b[1;32m   1438\u001b[0m     _test_batched_grad_forward_ad(func, tupled_inputs)\n",
      "File \u001b[0;32m~/gitrepo/torchgroup/.venv/lib/python3.8/site-packages/torch/autograd/gradcheck.py:1075\u001b[0m, in \u001b[0;36m_gradcheck_real_imag\u001b[0;34m(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps, rtol, atol, check_grad_dtypes, check_forward_ad, check_backward_ad, nondet_tol, check_undefined_grad)\u001b[0m\n\u001b[1;32m   1072\u001b[0m         gradcheck_fn(real_fn, real_func_out, tupled_inputs, real_outputs, eps,\n\u001b[1;32m   1073\u001b[0m                      rtol, atol, check_grad_dtypes, nondet_tol, complex_indices\u001b[39m=\u001b[39mcomplex_out_indices)\n\u001b[1;32m   1074\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1075\u001b[0m         gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,\n\u001b[1;32m   1076\u001b[0m                      rtol, atol, check_grad_dtypes, nondet_tol)\n\u001b[1;32m   1078\u001b[0m \u001b[39mif\u001b[39;00m check_forward_ad:\n\u001b[1;32m   1079\u001b[0m     complex_inp_indices \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i, inp \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tupled_inputs) \u001b[39mif\u001b[39;00m is_tensor_like(inp) \u001b[39mand\u001b[39;00m inp\u001b[39m.\u001b[39mis_complex()]\n",
      "File \u001b[0;32m~/gitrepo/torchgroup/.venv/lib/python3.8/site-packages/torch/autograd/gradcheck.py:1131\u001b[0m, in \u001b[0;36m_slow_gradcheck\u001b[0;34m(func, func_out, tupled_inputs, outputs, eps, rtol, atol, check_grad_dtypes, nondet_tol, use_forward_ad, complex_indices, test_imag)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[39mfor\u001b[39;00m j, (a, n) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(analytical, numerical[i])):\n\u001b[1;32m   1130\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _allclose_with_type_promotion(a, n\u001b[39m.\u001b[39mto(a\u001b[39m.\u001b[39mdevice), rtol, atol):\n\u001b[0;32m-> 1131\u001b[0m                 \u001b[39mraise\u001b[39;00m GradcheckError(_get_notallclose_msg(a, n, i, j, complex_indices, test_imag))\n\u001b[1;32m   1133\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mGradcheckError\u001b[0m: Jacobian mismatch for output 0 with respect to input 0,\nnumerical:tensor([[-5.8966e-03, -9.9945e-01,  3.0979e-03,  ...,  1.5226e-05,\n          1.1569e-05,  4.6796e-08],\n        [-1.0280e-02, -3.2028e-03, -9.9955e-01,  ..., -1.9792e-05,\n          1.8319e-05,  6.3222e-07],\n        [ 6.0973e-07, -5.0663e-03,  3.0390e-03,  ..., -2.5842e-05,\n         -6.0821e-06,  2.0206e-07],\n        ...,\n        [ 3.2169e-07, -1.5755e-05, -2.3154e-05,  ..., -5.0842e-03,\n          1.2298e-02,  2.8855e-03],\n        [-2.0595e-08, -1.4878e-05,  1.9309e-05,  ...,  9.9967e-01,\n          2.2427e-03,  3.8410e-03],\n        [-4.2927e-07, -1.1387e-05, -1.8292e-05,  ..., -2.2568e-03,\n          9.9968e-01,  4.1756e-04]], dtype=torch.float64)\nanalytical:tensor([[-5.8969e-03, -9.9998e-01,  3.1804e-03,  ...,  4.6177e-05,\n          3.4821e-05,  1.3579e-07],\n        [-1.0282e-02, -3.1191e-03, -9.9995e-01,  ..., -5.9665e-05,\n          5.3867e-05,  1.8906e-06],\n        [ 1.8334e-06, -5.1242e-03,  2.9816e-03,  ..., -7.6937e-05,\n         -1.8028e-05,  6.0946e-07],\n        ...,\n        [ 9.6899e-07, -4.7181e-05, -6.7861e-05,  ..., -5.1301e-03,\n          1.2228e-02,  2.8861e-03],\n        [-6.9706e-08, -4.4127e-05,  5.7611e-05,  ...,  9.9999e-01,\n          2.2483e-03,  3.8414e-03],\n        [-1.2943e-06, -3.4027e-05, -5.5952e-05,  ..., -2.2498e-03,\n          1.0000e+00,  4.1845e-04]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "n = 16\n",
    "device = 'cpu'\n",
    "g = SO(n,device=device)\n",
    "w = torch.randn(g.gdim,requires_grad=True,dtype=torch.double,device=device)*1e-2\n",
    "torch.autograd.gradcheck(g.element,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 0,\n",
      "numerical:tensor([[0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        ...,\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j]],\n",
      "       device='cuda:0', dtype=torch.complex128)\n",
      "analytical:tensor([[-1.2511e-34+5.3906e-34j, -9.5605e-36-8.5180e-36j,\n",
      "         -6.6165e-38+5.7559e-39j,  ...,\n",
      "          0.0000e+00-0.0000e+00j,  0.0000e+00-0.0000e+00j,\n",
      "          0.0000e+00-0.0000e+00j],\n",
      "        [ 4.8671e-33-7.1404e-34j, -1.1773e-34+5.0727e-34j,\n",
      "         -8.9865e-36-8.0065e-36j,  ...,\n",
      "          0.0000e+00-0.0000e+00j,  0.0000e+00-0.0000e+00j,\n",
      "          0.0000e+00-0.0000e+00j],\n",
      "        [ 3.3211e-31+2.5767e-31j,  4.5765e-33-6.7134e-34j,\n",
      "         -1.1058e-34+4.7645e-34j,  ...,\n",
      "          0.0000e+00-0.0000e+00j,  0.0000e+00-0.0000e+00j,\n",
      "          0.0000e+00-0.0000e+00j],\n",
      "        ...,\n",
      "        [ 0.0000e+00-0.0000e+00j,  0.0000e+00-0.0000e+00j,\n",
      "          0.0000e+00-0.0000e+00j,  ...,\n",
      "          1.1058e-34+4.7645e-34j, -4.5765e-33-6.7134e-34j,\n",
      "         -3.3211e-31+2.5767e-31j],\n",
      "        [ 0.0000e+00-0.0000e+00j,  0.0000e+00-0.0000e+00j,\n",
      "          0.0000e+00-0.0000e+00j,  ...,\n",
      "          8.9865e-36-8.0065e-36j,  1.1773e-34+5.0727e-34j,\n",
      "         -4.8671e-33-7.1404e-34j],\n",
      "        [ 0.0000e+00-0.0000e+00j,  0.0000e+00-0.0000e+00j,\n",
      "          0.0000e+00-0.0000e+00j,  ...,\n",
      "          6.6165e-38+5.7559e-39j,  9.5605e-36-8.5180e-36j,\n",
      "          1.2511e-34+5.3906e-34j]], device='cuda:0', dtype=torch.complex128)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 33\n",
    "device = 'cuda'\n",
    "g = GW()\n",
    "w = torch.randn(n,requires_grad=True,dtype=torch.double,device=device) +0j\n",
    "w[1:] = w[1:] + 1j*torch.randn(n-1,requires_grad=True,dtype=torch.double,device=device)\n",
    "w[1:] = w[1:]/torch.arange(1,n,dtype=torch.double,device=device)**2\n",
    "w[4:] = 0\n",
    "w = torch.cat([w[1:].flip([-1]).conj(),w])*1e-3\n",
    "\n",
    "try:\n",
    "    torch.autograd.gradcheck(g.element,w,atol=1e-2,rtol=1e-1)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4419e-14, dtype=torch.float64)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 16\n",
    "g = SO(n)\n",
    "w1 = torch.randn(g.gdim,dtype=torch.double)\n",
    "w2 = torch.randn(g.gdim,dtype=torch.double)\n",
    "l1 = g.algebra(w1)\n",
    "l2 = g.algebra(w2)\n",
    "\n",
    "c_al = l1@l2-l2@l1\n",
    "\n",
    "ad = g.adjoint(w1)\n",
    "w = ad@w2\n",
    "c_ad = g.algebra(w)\n",
    "\n",
    "torch.dist(c_al,c_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdouble\n\u001b[1;32m      4\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m g \u001b[39m=\u001b[39m SO(n,device\u001b[39m=\u001b[39;49mdevice)\n",
      "Cell \u001b[0;32mIn[117], line 24\u001b[0m, in \u001b[0;36mSO.__init__\u001b[0;34m(self, n, device)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__index_ad \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgdim,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgdim,dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong,device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgdim):\n\u001b[0;32m---> 24\u001b[0m     ad[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectorize(al[i]\u001b[39m@al\u001b[39m\u001b[39m-\u001b[39mal\u001b[39m@al\u001b[39;49m[i])\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     25\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__index_ad[ad[i]\u001b[39m!=\u001b[39m\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m i\n\u001b[1;32m     27\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__coef_ad \u001b[39m=\u001b[39m ad\u001b[39m.\u001b[39msum(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdouble,device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n = 35\n",
    "batch = 256\n",
    "dtype = torch.double\n",
    "device = 'cuda'\n",
    "g = SO(n,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1089, -0.0238,  0.0203,  ..., -0.4617, -0.2534, -0.1915],\n",
      "        [-0.8672,  0.3930, -0.1102,  ...,  0.0428,  0.2602, -0.9593],\n",
      "        [ 0.5985,  0.1622,  0.1053,  ...,  0.1972,  0.2944, -0.2771],\n",
      "        ...,\n",
      "        [-0.3459, -0.1368, -0.4822,  ...,  0.7324,  0.0742,  0.0045],\n",
      "        [-0.0965, -0.0478,  0.0599,  ..., -0.4932, -0.3087,  0.4541],\n",
      "        [ 0.6555,  0.2391, -0.8157,  ..., -0.0915, -1.3080,  0.5998]],\n",
      "       device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "w = torch.randn(batch,g.gdim,requires_grad=True,dtype=dtype,device=device)\n",
    "A = g.element(w)\n",
    "\n",
    "x = torch.randn(batch,g.mdim,10,dtype=dtype,device=device)\n",
    "y = torch.randn(batch,g.mdim,10,dtype=dtype,device=device)\n",
    "loss = torch.dist(y,A@x)\n",
    "loss.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2464829921.0388393\r"
     ]
    }
   ],
   "source": [
    "n = 2049\n",
    "batch = 256\n",
    "device = 'cuda'\n",
    "dtype = torch.complex128\n",
    "g = GW()\n",
    "w = torch.nn.Parameter(torch.zeros(n,dtype=dtype,device=device))\n",
    "x = torch.randn(batch,2*n-1,10,dtype=dtype,device=device)\n",
    "w_ = torch.randn(n,device=device,dtype=dtype)/torch.arange(1,n+1,device=device)**2*1e-3\n",
    "A = g.element(torch.cat([w_[1:].flip([-1]).conj(),w_[:1].real+0j,w_[1:]]))\n",
    "y = A@x\n",
    "optimizer = torch.optim.Adam([w],lr=1e-4)\n",
    "\n",
    "# w[1:] = w[1:] + 1j*torch.randn(n-1,requires_grad=True,dtype=torch.double,device=device)\n",
    "# w[1:] = w[1:]/torch.arange(1,n,dtype=torch.double,device=device)\n",
    "for i in range(1000):\n",
    "    A = g.element(torch.cat([w[1:].flip([-1]).conj(),w[:1].real+0j,w[1:]]))\n",
    "    loss = torch.dist(y,A@x)\n",
    "    print(f'{loss}\\r', end='')\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

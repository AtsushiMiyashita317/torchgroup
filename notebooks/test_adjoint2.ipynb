{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mytorch\n",
    "import mytorch.math as math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torchgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LieGroup(mytorch.autograd.Function):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def _create_impl(self, pade_order, dtype=None, device=None):\n",
    "        assert pade_order>=2\n",
    "        # pade coefficient of (1-e^{-x})/x\n",
    "        k = torch.arange(1,2*pade_order,dtype=dtype)\n",
    "        c = -torch.cumprod(-1/k,-1)\n",
    "        a1,b1 = math.pade(c,pade_order,pade_order)\n",
    "        # pade coefficient of e^{-x}\n",
    "        k = torch.arange(2*pade_order-1,dtype=dtype)\n",
    "        k[0] = 1\n",
    "        c = -torch.cumprod(-1/k,-1)\n",
    "        a2,b2 = math.pade(c,pade_order,pade_order)\n",
    "        \n",
    "        c = torch.stack(\n",
    "            [\n",
    "                torch.stack([a1,b1],dim=-1),\n",
    "                torch.stack([a2,b2],dim=-1)\n",
    "            ],\n",
    "            dim=-1\n",
    "        ).to(device=device)\n",
    "        \n",
    "        class impl(torch.autograd.Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx:torch.autograd.function.FunctionCtx, w:torch.Tensor):\n",
    "                al = self.algebra(w.detach())\n",
    "                y = torch.matrix_exp(al)\n",
    "                ctx.save_for_backward(y,w.detach())\n",
    "                return y\n",
    "            \n",
    "            @staticmethod\n",
    "            def backward(ctx:torch.autograd.function.FunctionCtx, dy:torch.Tensor):\n",
    "                y,w = ctx.saved_tensors\n",
    "                ad = self.adjoint(w)\n",
    "                e = torch.eye(ad.size(-1),dtype=ad.dtype,device=ad.device)\n",
    "                c_ = c[(...,)+(None,)*ad.ndim]\n",
    "                # scaling\n",
    "                n = torch.log2(ad.abs().max()*ad.size(-1)).ceil().int().maximum(torch.tensor(0))\n",
    "                ad = ad/(2**n)\n",
    "                # pade approximation\n",
    "                r = c_[0]*e+c_[1]*ad\n",
    "                p = ad\n",
    "                for i in range(2,pade_order):\n",
    "                    p = ad@p\n",
    "                    r = r+c_[i]*p\n",
    "                r = r[0]@torch.inverse(r[1])\n",
    "                # squaring\n",
    "                for _ in range(n):\n",
    "                    r[0] = r[0]@(e+r[1])/2\n",
    "                    r[1] = r[1]@r[1]\n",
    "                # chain rule\n",
    "                dw = self.derivative(y, dy.conj())@r[0]\n",
    "                # dw = torch.einsum('...ij,...ik,...mn,mkj->...n',dy,y,r,self.al)\n",
    "                return dw.conj()\n",
    "            \n",
    "        return impl.apply\n",
    "            \n",
    "    def algebra(self, w:torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError(f\"LieGroup [{type(self).__name__}] is missing the required \\\"algebra\\\" function\")\n",
    "    \n",
    "    def adjoint(self, w:torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError(f\"LieGroup [{type(self).__name__}] is missing the required \\\"adjoint\\\" function\")\n",
    "    \n",
    "    def derivative(self, y:torch.Tensor, dy:torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError(f\"LieGroup [{type(self).__name__}] is missing the required \\\"derivative\\\" function\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SO(torchgroup.lie.LieGroup):\n",
    "    def __init__(self, n:int, device=None) -> None:\n",
    "        super().__init__()\n",
    "        self.mdim = n\n",
    "        self.gdim = n*(n-1)//2\n",
    "        \n",
    "        al = torch.zeros(self.gdim,self.mdim,self.mdim,dtype=torch.int)\n",
    "        self.__index_al = torch.zeros(self.mdim,self.mdim,dtype=torch.long,device=device)\n",
    "        k = 0\n",
    "        for i in range(1,n):\n",
    "            for j in range(i):\n",
    "                al[k,i,j] = 1\n",
    "                al[k,j,i] = -1\n",
    "                self.__index_al[i,j] = k\n",
    "                self.__index_al[j,i] = k\n",
    "                k += 1\n",
    "        \n",
    "        self.__coef_al = al.sum(0).to(dtype=torch.double,device=device)\n",
    "        self.__index_al = self.__index_al.flatten()\n",
    "        \n",
    "        ad = torch.zeros(self.gdim,self.gdim,self.gdim,dtype=torch.int)\n",
    "        self.__index_ad = torch.zeros(self.gdim,self.gdim,dtype=torch.long,device=device)\n",
    "        for i in range(self.gdim):\n",
    "            ad[i] = self.vectorize(al[i]@al-al@al[i]).transpose(-2,-1)\n",
    "            self.__index_ad[ad[i]!=0] = i\n",
    "        \n",
    "        self.__coef_ad = ad.sum(0).to(dtype=torch.double,device=device)\n",
    "        self.__index_ad = self.__index_ad.flatten()\n",
    "        \n",
    "        self._set(self._create_impl(dtype=torch.double,device=device))\n",
    "        \n",
    "    def vectorize(self, x:torch.Tensor):\n",
    "        idx0,idx1 = mytorch.count_to_index(torch.arange(self.mdim))\n",
    "        return x[...,idx0,idx1]\n",
    "            \n",
    "    def algebra(self, w:torch.Tensor):\n",
    "        return w[...,self.__index_al].unflatten(-1,(self.mdim,self.mdim))*self.__coef_al\n",
    "        # return torch.einsum('...i,ijk->...jk',w,self.al)\n",
    "    \n",
    "    def adjoint(self, w:torch.Tensor):\n",
    "        return w[...,self.__index_ad].unflatten(-1,(self.gdim,self.gdim))*self.__coef_ad\n",
    "        # return torch.einsum('...i,ijk->...jk',w,self.ad)\n",
    "    \n",
    "    def derivative(self, y:torch.Tensor, dy:torch.Tensor):\n",
    "        return torch.zeros(\n",
    "            y.size()[:-2]+(self.gdim,), \n",
    "            dtype=y.dtype, \n",
    "            device=y.device\n",
    "        ).index_add_(\n",
    "            -1,\n",
    "            self.__index_al,\n",
    "            ((y.transpose(-2,-1)@dy)*self.__coef_al).flatten(-2)\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GW(torchgroup.lie.LieGroup):\n",
    "    def __init__(self, device=None) -> None:\n",
    "        super().__init__()\n",
    "        self._set(self._create_impl(device=device))\n",
    "        \n",
    "    def coef_al(self, dim:int, device=None):\n",
    "        return -1j*(torch.abs(torch.arange(dim,device=device)[:,None]-torch.arange(dim,device=device)[None,:])<=(dim-1)//2)*\\\n",
    "            torch.arange(-dim//2+1,dim//2+1,device=device)\n",
    "    \n",
    "    def index_al(self, dim:int, device=None):\n",
    "        i = torch.nn.functional.pad(\n",
    "                torch.arange(\n",
    "                    dim,\n",
    "                    device=device,\n",
    "                    dtype=torch.long   \n",
    "                ),\n",
    "                [(dim-1)//2,(dim-1)//2]\n",
    "            )\n",
    "        return torch.as_strided(\n",
    "            i,\n",
    "            size=(dim,dim),\n",
    "            stride=(i.stride(-1),i.stride(-1))\n",
    "        ).flip([-1]).flatten()\n",
    "            \n",
    "    def algebra(self, w:torch.Tensor):\n",
    "        dim = w.shape[-1]\n",
    "        w_ = torch.nn.functional.pad(w,[(dim-1)//2,(dim-1)//2])\n",
    "        return torch.as_strided(\n",
    "            w_,\n",
    "            size=w_.shape[:-1]+(dim,dim),\n",
    "            stride=w_.stride()[:-1]+(w_.stride(-1),w_.stride(-1))\n",
    "        ).flip([-1]).mul(\n",
    "            -1j*torch.arange(-dim//2+1,dim//2+1,device=w.device)\n",
    "        )\n",
    "        \n",
    "    def adjoint(self, w:torch.Tensor):\n",
    "        dim = w.shape[-1]\n",
    "        w_ = torch.nn.functional.pad(w,[(dim-1)//2,(dim-1)//2])\n",
    "        c = torch.arange(-3*(dim-1)//2,3*(dim-1)//2+1,device=w.device)\n",
    "        return torch.as_strided(\n",
    "                w_,\n",
    "                size=w_.shape[:-1]+(dim,dim),\n",
    "                stride=w_.stride()[:-1]+(w_.stride(-1),w_.stride(-1))\n",
    "            ).flip([-1])*\\\n",
    "            torch.as_strided(\n",
    "                -1j*c,\n",
    "                size=(dim,dim),\n",
    "                stride=(c.stride(-1),2*c.stride(-1))\n",
    "            ).flip([-2])\n",
    "    \n",
    "    def derivative(self, y:torch.Tensor, dy:torch.Tensor):\n",
    "        return torch.zeros(\n",
    "            y.size()[:-2]+(y.size(-1),), \n",
    "            dtype=y.dtype, \n",
    "            device=y.device\n",
    "        ).index_add_(\n",
    "            -1,\n",
    "            self.index_al(y.size(-1), device=y.device),\n",
    "            ((y.transpose(-2,-1)@dy)*self.coef_al(y.size(-1), device=y.device)).flatten(-2)\n",
    "        )\n",
    "    \n",
    "    def element_(self, w: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.matrix_exp(self.algebra(w))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 16\n",
    "device = 'cpu'\n",
    "g = SO(n,device=device)\n",
    "w = torch.randn(g.gdim,requires_grad=True,dtype=torch.double,device=device)*1e+3\n",
    "torch.autograd.gradcheck(g.__call__,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 0,\n",
      "numerical:tensor([[0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        ...,\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j]],\n",
      "       device='cuda:0', dtype=torch.complex128)\n",
      "analytical:tensor([[nan+nanj, nan+nanj, nan+nanj,  ..., nan+nanj, nan+nanj, nan+nanj],\n",
      "        [nan+nanj, nan+nanj, nan+nanj,  ..., nan+nanj, nan+nanj, nan+nanj],\n",
      "        [nan+nanj, nan+nanj, nan+nanj,  ..., nan+nanj, nan+nanj, nan+nanj],\n",
      "        ...,\n",
      "        [nan+nanj, nan+nanj, nan+nanj,  ..., nan+nanj, nan+nanj, nan+nanj],\n",
      "        [nan+nanj, nan+nanj, nan+nanj,  ..., nan+nanj, nan+nanj, nan+nanj],\n",
      "        [nan+nanj, nan+nanj, nan+nanj,  ..., nan+nanj, nan+nanj, nan+nanj]],\n",
      "       device='cuda:0', dtype=torch.complex128)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 8\n",
    "device = 'cuda'\n",
    "g = GW(device=device)\n",
    "w = torch.randn(n,requires_grad=True,dtype=torch.double,device=device) +0j\n",
    "w[1:] = w[1:] + 1j*torch.randn(n-1,requires_grad=True,dtype=torch.double,device=device)\n",
    "w[1:] = w[1:]/torch.arange(1,n,dtype=torch.double,device=device)\n",
    "w[4:] = 0\n",
    "w = torch.cat([w[1:].flip([-1]).conj(),w])*0\n",
    "\n",
    "try:\n",
    "    torch.autograd.gradcheck(g.__call__,w)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 64\n",
    "batch = 32\n",
    "device = 'cuda'\n",
    "dtype = torch.complex64\n",
    "g = GW(device=device)\n",
    "x = torch.randn(batch,2*n+1,10,dtype=dtype,device=device)\n",
    "w_ = torch.randn(batch,n,device=device,dtype=dtype)/torch.arange(1,n+1,device=device)*1e-3\n",
    "A = g(torch.cat([w_.flip([-1]).conj(),torch.zeros_like(w_[...,:1]),w_],dim=-1))\n",
    "y = A@x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0079725272953510285\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdist(y,A\u001b[39m@x\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39m'\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     12\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     13\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/gitrepo/torchgroup/.venv/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/gitrepo/torchgroup/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w = torch.nn.Parameter(torch.zeros(batch,n,dtype=dtype,device=device))\n",
    "optimizer = torch.optim.Adam([w])\n",
    "\n",
    "# w[1:] = w[1:] + 1j*torch.randn(n-1,requires_grad=True,dtype=torch.double,device=device)\n",
    "# w[1:] = w[1:]/torch.arange(1,n,dtype=torch.double,device=device)\n",
    "for i in range(10000):\n",
    "    w_ = w/torch.arange(1,n+1,device=device)*1e-2\n",
    "    A = g(torch.cat([w_.flip([-1]).conj(),torch.zeros_like(w_[...,:1]),w_],dim=-1))\n",
    "    loss = torch.dist(y,A@x)\n",
    "    print(f'{loss}\\r', end='')\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
